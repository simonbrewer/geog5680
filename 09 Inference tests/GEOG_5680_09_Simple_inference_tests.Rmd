---
title: "GEOG 5680 6680 09 Simple inference tests in R"
author: | 
  | Simon Brewer
  | Geography Department
  | University of Utah
date: "May 18, 2021"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(png)
library(grid)
set.seed(42)
```

# Introduction

In this lab, we will introduce R functions for working with probability distributions and dealing with basic inference tests. 

Remember to use RStudio's script editor (top left panel) to enter your commands and then copy-paste these to the console when you are ready. This will keep a copy of the commands that you can save and return to. 

Before starting, remember to create a working directory (e.g. `module09`). Next download the files for today's lab from the Canvas page and move them to this directory. You'll need the following file:

- *Deer.csv*

Then start RStudio, and change your working directory from the [Session] menu > [Set working directory] > [Choose directory...]. Remember to check that R can see the files by running the `list.files()` command. 

# Distributions
The standard installation of R comes with about 20 standard statistical distributions (e.g. normal, uniform, binomial, beta, gamma, etc). Each of the distributions

Each distribution has four associated functions:

- `d*` - density functions (e.g. `dnorm()`)
- `p*` - probability distribution functions (e.g. `pnorm()`)
- `q*` - quantile functions (e.g. `qnorm()`)
- `r*` - random number generation (e.g. `rnorm()`)

We will look briefly at examples with a) the Poisson distribution as an example of a discrete probability distribution and b) the normal distribution as an example of a continuous distribution. For information on the other distributions and the parameters that they require, type: `help(Distributions)`.

## Poisson distribution

The Poisson distribution is used to describe count data, e.g. number of cars on a road, number of fires in a year, number of people in a household. The main parameter for the Poisson distribution is $\lambda$, the average count across observations. The probability of observing a count of $k$ in a given observation is then given by:

$$p(k) = \frac{\lambda^k e^{-\lambda}}{k!}$$

### `d*` functions

The `d`-functions gives the probability density for a given value. These are useful for comparing relative probability densities for two or more values of the variable of interest, and visualizing the distribution. `d`-functions require as input:

- a vector (or scalar) of values for which we want to probability density calculated
- one or more parameters controlling the distribution

For the Poisson distribution, the only distributional parameter is $\lambda$, the mean count. We will start by generating density values using `dpois()` for counts of 0 to 10, with $\lambda = 4$

```{r fig.keep='high'}
xx = seq(0,10)
pp = dpois(xx, 4)
barplot(pp, names=xx, main="dpois lambda=4",
        xlab = "x", ylab = "p(x)")
```

And we can use this to obtain the probability of a count of 2 in a random sample:

```{r fig.keep='none'}
dpois(2, 4)
```

### `p*` functions

These functions return the probability of being equal to, or lower than a given value in the distribution by default. This is effectively the *cumulative* probability distribution of obtaining that value or lower. For the Poisson distribution, we obtain this with `ppois()`, which again takes two parameters

- lambda
- the value of $x$

And it will return the probability of obtaining a random value less than or equal to $x$. For example, the probability of getting a count of two or lower with $\lambda=4$ is:

```{r}
ppois(2,4)
```

By default then, this returns the *lower tail* of the distribution. If we want to know the *upper tail*, i.e. the probability of being above a value, we can set the parameter `lower.tail` to FALSE:

```{r results='hide'}
ppois(2,4, lower.tail = FALSE)
```

Which is 1 less the probability of the lower tail. 
```{r results='hide'}
1-ppois(2,4, lower.tail = TRUE)
```

### `q*` functions

The `q`-functions provide the inverse to the `p`-functions, providing the value of $x$ corresponding to a given percentile of the distribution. This can be used to answer what values of $x$ would we expect 10\% of a distribution to fall below. For the Poisson distribution, the function is `qpois()`, and this provides the value of $x$ which would be greater than 5% of random samples for $\lambda=4$:

```{r results='hide'}
qpois(0.05, 4)
```

So 5% of samples will be `r qpois(0.05, 4)` or lower. The first parameter is the percentile and must therefore be between 0 and 1. Note that this also takes the `lower.tail` parameter. If this is set to TRUE (default), then the value returned is the value of $x$ which is greater than the given percentile. If FALSE, then this gives the value which is less than a given percentile. Compare the previous result to:
```{r results='hide'}
qpois(0.05, 4, lower.tail=FALSE)
```

Which tells us that 5% of samples will be `r qpois(0.05, 4)` or higher.

### `r*` functions

`r`-functions provide random number generation, with numbers drawn from a given distribution. The likelihood of a number being drawn is driven by the shape of the probability curve, and corresponds to the density function. These functions are extensively used in randomization procedures and Monte Carlo methods.

These functions require a parameter given the number of random numbers to be drawn, plus distribution parameters. Try running the following line several times to get a single random draw from a Poisson distribution with $\lambda=4$:
```{r results='hide'}
rpois(1, 10)
```

To demonstrate that these numbers are taking from a given distribution, we can plot a histogram of a large number of random numbers with the theoretical density. We will use a Poisson distribution with $\lambda=10$:
```{r fig.keep='none'}
xx.r = rpois(1000, 10)
hist(xx.r, breaks=seq(0,25), freq=FALSE)
xx.u = seq(0,25,by=1)
pp = dpois(xx.u, 10)
lines(xx.u, pp)
```

## Normal distribution
For the normal distribution, the functions are written as `*norm()`, and these take two distribution parameters:

- the mean (0 by default)
- the standard deviation (1 by default)

The probability density for a value of $x$ with these parameters is given by:
$$
p(x) = \frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{(x-\mu)^2}{2 \sigma^2}}
$$

### `d*` functions

The `d`-functions gives the density function. As before we need to give a value, or vector of values of $x$, for which we calculate the probability density. This gets density values for (and plots) the standard normal distribution:

```{r fig.keep='high'}
xx = seq(-3,3,by=0.05)
pp = dnorm(xx, 0, 1)
plot(xx, pp, type='l', main="dnorm, mean=0, sd=1",
        xlab = "x", ylab = "p(x)")
```

Again, we can use this to examine the relative probability densities of two values given a distribution. If we have distribution of temperatures with mean = 10 and s.d. = 5, and two observations (2.7 and 12.4), their densities are:
```{r}
xx = c(2.7, 12.4)
dnorm(xx, 10, 5)
```

These densities equate to the likelihood of these values given a probability distribution, and this forms the basis for maximum likelihood approaches and Bayesian inference. 

### `p*` functions

The following returns the probability of obtaining a value of 0 or lower in the normal standard distribution (mean = 0, s.d. = 1), which not too surprisingly is 0.5 as the normal distribution is symmetrical around zero:
```{r}
pnorm(0, mean=0, sd=1)
```

Now try, finding the probability of obtaining a value lower than or equal to 27.4 from a normal distribution with mean=50 and s.d.=20. Remember that we do not need to specify the parameter names, as long as we keep the expected order of the parameters (see `help(pnorm)` for the parameter order):
```{r results='hide'}
pnorm(27.4, 50, 20)
```

What is the probability of obtaining a value of $x>19$ when $x$ is normally distributed, with mean=17.46 and s.d.=19.38? Note that by default this gives the probability of being lower than the value specified. As we want the probability of being greater than the value, we need to set the parameter `lower.tail` to FALSE:

```{r results='hide'}
pnorm(19, 17.46, 19.38, lower.tail = FALSE)
```

We can also plot out the distribution function as described by `pnorm()`. This is a very useful way to visualize the relationship between values and probabilities. As the `p*` functions return a probability, we need to use a vector of values corresponding to the distribution, and for this we will use the `seq()` function. We will plot out the distribution for the standard normal (mean=0,s.d.=1), so we need a vector of values corresponding to approximately `3 * s.d.`:
```{r fig.keep='none'}
xx = seq(-3,3,by=0.05)
pp = pnorm(xx, 0, 1)
plot(xx, pp, type='l', main="pnorm, mean=0, sd=1")
```

### `q*` functions

The `q`-functions provide the inverse to the `p`-functions. In other words, this calculates the value corresponding to a given percentile of the distribution. To know what value corresponds to 90% of a normal distribution with mean=4000 and s.d.=250 (i.e. we expect 90% of values to be lower than this)
```{r results='hide'}
qnorm(0.9, 4000, 250)
```

The first value must therefore be between 0 and 1. Again we can plot out the distribution, which will show the inverse of the previous graphic. Note that now we need a vector of percentiles, not values. 
```{r fig.keep='none'}
qq = seq(0,1,by=0.05)
pp = qnorm(qq, 0, 1)
plot(qq, pp, type='l', main="qnorm, mean=0, sd=1")
```

### `r*` functions

`r`-functions provide random number generation, with numbers drawn from a given distribution. The likelihood of a number being drawn is driven by the shape of the probability curve, and corresponds to the density function. These functions are extensively used in randomization procedures and Monte Carlo methods.

These functions require a parameter given the number of random numbers to be drawn, plus mean/s.d. or other information about the shape. Try running the following line several times:
```{r results='hide'}
rnorm(1,0,1)
```

As before, we can plot a histogram of a large number of random numbers with the theoretical density. We will use a normal distribution with mean = 200, s.d. = 10:
```{r fig.keep='none'}
xx.r = rnorm(1000,mean=200,sd=10)
hist(xx.r, breaks=40, freq=FALSE)
xx.u = seq(170,230,by=1)
pp = dnorm(xx.u, 200, 10)
lines(xx.u, pp)
```

# Inference tests

## The *t*-test

Our first example of a statistical test is based on the comparison of means, using a *t*-test. We will use the example presented in class to test if the heights of actors auditioning for the roles of Aragorn and Gimli in the Lord of the Rings films are similar , or if there is a significant difference. 

We don't, obviously, have the information for the actors who auditioned, so we will make up two random sets of heights, using the `r*` functions introduced above. We will first simulate the heights of 50 Aragorn actors using a random normal distribution, with a mean of 180cm and a standard deviation of 10cm:
```{r}
aragorn = rnorm(50, mean=180, sd=10)
```

And now the same for the Gimli actors, with a mean height of 132cm, and a sd of 15cm:
```{r}
gimli = rnorm(50, mean=132, sd=15)
```

Using the plotting examples from the previous day, make up histograms of the two sets of heights. 

Now we first form our null and alternative hypothesis. We will use a two-sided *t*-test as we are looking for differences that may be greater or smaller than the expected value.

- Null $H_0$: The two groups of actors have the same height
- Alternative $H_a$: The two groups of actors have different heights

Now we run the test, using the `t.test()` function. This requires two vector of values for the two sample *t*-test (or a vector and a value $\mu$ for a one sample test). We also set the parameter `alternative`, which states whether this is two-sided or one-sided. As our alternative hypothesis is simply that the two sets of actors are *different*, we use the two-sided test:

```{r}
t.test(aragorn, gimli, alternative="two.sided")
```

There is a lot of information in the output, but the two values we are most interested in are *t* (=$`r round(t.test(aragorn, gimli, alternative="two.sided")$statistic,4)`$) and *p* (=$`r round(t.test(aragorn, gimli, alternative="two.sided")$p.value,4)`$). The low value of *p* means we can reject out null hypothesis with high confidence (this actually suggests that the chance of making a mistake in our conclusion is so small it can't be calculated). 

We might wish to restate our hypothesis - after all we know that Gimli is a dwarf. So we can run a one-sided test in which our alternative hypothesis is now that the Gimli actors heights are less than the Aragorn actors. For the `t.test` function, we ask if the first vector is greater than the second for a one-sided test:

```{r}
t.test(aragorn, gimli, alternative="greater")
```

## Analysis of variance

If we need to test the difference in the means between more that two groups, we can use an analysis of variance (ANOVA). Here, we look at the ratio of variance between the groups, over the variance within the groups. This ratio follows an *F*-distribution, where large values indicate substantial differences between groups, and we can use this to test its significance. 

We will run it here on the three groups of actors created in the previous step (you will need to have done exercise 1). There are two functions for calculating ANOVA in R. Here we will use `aov()`. This requires a single vector of all the heights, and a matching vector which assigns each observation to one of the groups. Code to create this is at the end of this document, but you should be able to do this using the material we have covered in previous labs. 

Once you have the two vectors, the syntax for the ANOVA is as follows:

```{r echo=FALSE}
legolas = rnorm(50, 195, 15)
actors = c(rep("Aragorn", 50), rep("Gimli", 50), rep("Legolas", 50))
heights = c(aragorn, gimli, legolas)
fit = aov(heights ~ actors)
```
```{r}
aov(heights ~ actors)
```

This provides information about how the variance is partitioned in the groups, with $`r summary(fit)[[1]]["Sum Sq"][1,1]`$ explained by the groups, and $`r summary(fit)[[1]]["Sum Sq"][2,1]`$ in residual variance. In order to test if this is significant, you can just use the `summary()` function:

```{r}
summary(aov(heights ~ actors))
```

We now have extra information: the 'Mean Sq` gives the variance averaged across the degrees of freedom, and the 'F value' is the ratio of the two 'Mean Sq' values. The *p*-value is very low, indicating a significant difference. This should not be too surprising after the *t*-tests, but here we have tested all groups together, so we avoid the problem of carrying out multiple comparisons. 

## The *F*-test

An alternative test is to ask if the observations coming from two different sample sets have the same variance. For this, we use the *F*-test, a test of the ratio of the variances of the two sets. If this ratio is close to one, then it is likely that they have the same variance. If the ratio is much larger or smaller, then it is likely that the variances are different. 

We can test the samples of actors to see if the variances in their heights differ, by using the function `var.test()`. 

```{r}
var.test(aragorn,gimli)
```

In this test, the value of *p* is high (=$`r round(var.test(aragorn,gimli)$p.value,4)`$), suggesting that there is *no* significant difference in variance. 

## Correlation tests
Correlation in R is provided by the `cor()` function. However, this simply calculates the correlation value using Pearson's, Kendall's or Spearman's formula, here for two of the variables in the Iris dataset: 

```{r}
cor(iris$Sepal.Length, iris$Sepal.Width)
```

To test if the correlation is significantly different from correlation resulting from randomly sampled, unassociated variables, the correlation can be converted to a *t*-statistic, then tested against a *t*-distribution with *n*-2 degrees of freedom. In R, we can do this using the `cor.test()` function:

```{r}
cor.test(iris$Sepal.Length, iris$Sepal.Width)
```

## Chi-squared tests

Chi-squared tests are used to examine if the distribution of observations among classes (or factors) is uniform or as close to uniform as might be expected with sampling error. 

We'll examine this using the data in *Deer.csv*. Load this file into your workspace and use `table()` to look at the number of animals of both sexes:

```{r}
deer = read.csv("Deer.csv")
table(deer$Sex)
```

And you will see that there are more male (1) deer recorded than female (2). To test if this is a significance difference, we can run the chi-squared test on the table output:

```{r}
chisq.test(table(deer$Sex))
```

The same function can be used to test for association between groups, i.e. if we have one factor (A,B) and a second (X,Y), we might be interested to know if the X's a preferentially found with A's. The deer dataset has a binary variable 'Tb' which records the presence or absence of bovine tuberculosis in the deer. We might then ask if tuberculosis is preferentially found in males or females, or is equally spread among the sexes. Start by making the cross table of sex and tuberculosis:

```{r results='hide'}
table(deer$Sex, deer$Tb)
```

While there are many more non-TB deer than TB deer, the differences among female and male are small. We can again test this with the `chisq.test()` function to see if these difference are large enough to be considered significant:
```{r results='hide'}
chisq.test(table(deer$Sex, deer$Tb))
```

# Exercises

This exercise will get you to repeat several of the tests introduced above. Your answer should consist of your R code and answers to the questions given, including the output of the R functions. You can either use a Word document to record all of this, or use R Markdown to generate a html page containing all results and answers (recommended).

- Make up a vector of 50 random Legolas actors, with mean height of 195cm, and a standard deviation of 15cm. Run a *t*-test to compare this sample of actors to the set of Aragorns and then the set of Gimlis. Do you find evidence for significant differences?
- Re-run the variance test (*F*-test) to compare the group of Gimli and Legolas actors. Do these groups have different variance?
- Redo the correlation for the Sepal Length and Sepal Width for the Iris dataset, but for the three individual species. Are these correlated?
- Using the deer dataset and the `chisq.test()` function, test:
    - If there are significant differences in the number of deer caught per month
    - If the cases of tuberculosis are uniformly distributed across all farms

# Files used in lab

## *Deer.csv*

 Spanish deer health dataset

| Column header | Variable |
| --- | --- |
| Farm | Farm ID |
| Month | Farm ID |
| Year | Farm ID |
| Sex | Farm ID |
| clas1_4 | Age class |
| LCT | Length |
| KFI | Kidney fat index |
| Ecervi | Elaphostrongylus cervi presence (0/1) |
| Tb | Tuberculosis presence (0/1) |

## *iris.csv* 

Fisher's Iris morphology dataset

| Column header | Variable |
| --- | --- |
| Sepal.Length | Sepal length (mm) |
| Sepal.Width | Sepal width (mm) |
| Petal.Length | Petal length (mm) |
| Petal.Width | Petal width (mm) |
| Species | Species name |
| Code | Species code |

