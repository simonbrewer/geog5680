---
title: "GEOG 5680 6680 12 Spatial data in R"
author: | 
  | Simon Brewer
  | Geography Department
  | University of Utah
date: "May 02, 2020"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(png)
library(grid)
```

## Introduction

In this lab, we will explore some of R's functions for working with spatial data. R has been extensively developed to work with spatial data, and this is one thing that distinguishes from many other analytical languages. You should be aware, however, that R is currently transitioning away from the data formats described in this document to use a new set based on [simple features][sfID]. However, as this is still in progress, and as the majority of the spatial analytical tools (e.g. spatial regression, hotspot analysis) do not yet use these formats, we'll stick here with the oder formats based on the **sp** package.

Remember to use RStudio's script editor (top left panel) to enter your commands and then copy-paste these to the console when you are ready. This will keep a copy of the commands that you can save and return to. 

Before starting, remember to create a working directory (e.g. `module12`). Next download the files for today's lab from the Canvas page and move them to this directory. You'll need the following files:

- Climate dataset for Western North America: *WNAclimate.csv*
- Temperature dataset for Oregon in shapefile format: *oregon.zip*
- New York state polygon data in a shapefile: *NY_Data.zip*
- A shapefile of country borders: *ne_50m_admin_0_countries.zip*
- A shapefile of Switzerland: *borders.zip*
- Digital elevation map from Switzerland: *swiss_dem.grd*
- A NetCDF file of global monthly air temperature: *air.mon.ltm.nc*

Make sure that you unzip the zip files. As a warning - sometimes Windows adds an extra recursive folder when it extracts files from a zip file. For example, if you unzip the file called *borders.zip* this should make a folder called `borders` with some files in it. Windows may make a double directory (`borders\borders`). If R can't find some of the files in the examples, it might be worth checking this. 

Base R has no structure for spatial data, so you will need to install the following packages:

- **sp**
- **maptools**
- **rgdal**
- **raster**
- **RColorBrewer**
- **classInt**
- **ggmap**

## Spatial Objects

The **sp** package provides S4 spatial objects - these are R objects that can hold spatial information (e.g. coordinates, projection information) as well as data. There are several of these objects, and the following section introduces the main types. 

### SpatialPoint* Objects

Read in the WNA Climate dataset from the csv file *WNAclimate.csv*, and use the `names()` function to see the columns in the dataframe:

```{r}
wnaclim <- read.csv("WNAclimate.csv")
names(wnaclim)
```

We now create a simple SpatialPoint object of the location of the sites in the dataframe using the `SpatialPoints()` function. Note that we first create a coordinate reference system (CRS) for the data (here, simple longitude/latitude on a WGS84 sphere).

```{r message=FALSE}
library(rgdal)
llCRS <- CRS("+proj=longlat +ellps=WGS84")
wnaclim.sp <- SpatialPoints(cbind(wnaclim$LONDD, wnaclim$LATDD), 
                            proj4string=llCRS)
summary(wnaclim.sp)
```

There are some standard functions to obtain basic data and metadata about any Spatial* object:
```{r results='hide'}
bbox(wnaclim.sp)
proj4string(wnaclim.sp)
coordinates(wnaclim.sp)
```

The original dataframe also contains various other parameters (elevation, climate, site name) in columns that we can associated with the SpatialPoints, by creating a SpatialPointsDataFrame.
```{r results='hide'}
wnaclim.spdf <- SpatialPointsDataFrame(cbind(wnaclim$LONDD, 
                                             wnaclim$LATDD), 
                                       wnaclim, proj4string=llCRS)
summary(wnaclim.spdf)
```

Now we have a spatial object with associated values, we can make some plots. Using R's basic `plot()` function with any spatial data will simply plot the spatial geometry:

```{r fig.keep='none'}
plot(wnaclim.spdf)
```

The **sp** package has a function to make thematic maps. For this we need to specify the Spatial* object and the column name or attribute to use. Note that both this and the `plot()` function will adapt to different spatial data types. 

```{r fig.keep='none'}
spplot(wnaclim.spdf, "Jan_Tmp", colorkey=TRUE)
spplot(wnaclim.spdf, "Jul_Tmp", colorkey=TRUE)
```

As the attributes for these locations are held as a data frame, we can use all the standard R functions with them. For example, to calculate the mean and standard deviation of January temperatures:

```{r}
mean(wnaclim.spdf$Jan_Tmp)
sd(wnaclim.spdf$Jan_Tmp)
```

And to plot a histogram of annual precipitation:

```{r fig.keep='none'}
hist(wnaclim.spdf$annp, main="W.NAm Annual Precipitation", breaks=20)
```

### SpatialPolygon* Objects

You can create your own polygonss using a combination of the `Polygon()` and the `SpatialPolygon()` function. However, it is generally easier to import polygons that have been created by GIS software. Shapefiles are one of the most commonly used formats, and the **rgdal** package includes a function (`readOGR()`) for reading these directly into R. This fucntion will read most standard spatial data formats, and you can see the full set of vector formats that can be read by typing `ogrDrivers()` at the console. 

The zip file `NY_data.zip` contains shapefiles for part of the state of New York. Unzip this in your current working directory, then read in the data as follows:

```{r results='hide', fig.keep='none', message=FALSE}
NY8 <- readOGR("NY_data/NY8_utm18.shp")
class(NY8)
summary(NY8)
```

This creates a SpatialPolygonDataFrame; a polygon object with spatial attributes and an associated data frame. As the shapefile had an associated file with projection information (a `*.prj` file), the projection is automatically set in R.

```{r results='hide', fig.keep='none', message=FALSE}
proj4string(NY8)
```

Which tells us that file is in a UTM Zone 18 projection. As in the previous example, this object has an associated data frame containing attributes, and we can use this like any standard R data frame. If, for example, we want to calculate the total population:

```{r}
sum(NY8$POP8)
```

We can again use `spplot()` to make a thematic map by specifying which column or attribute we want to use:

```{r results='hide', message=FALSE}
spplot(NY8, "POP8")
```

If we are only interested in a subset of the data, we can use the attributes to select polygons. The data frame contains a column with the district associated with each polygon. If we only want the subset corresponding to Syracuse, we can extract this as follows:

```{r results='hide', fig.keep='none', message=FALSE}
Syracuse <- NY8[NY8$AREANAME == "Syracuse city", ]
plot(Syracuse)
spplot(Syracuse, "POP8")
```

Or we can select by an attribute. For example to extract only the polygons with over 15% population ages 65 and over (this is the variable 'PCTAGE65P'):

```{r results='hide', fig.keep='none', message=FALSE}
Pop65p <- Syracuse[Syracuse$PCTAGE65P > 0.15, ]
plot(Pop65p)
spplot(Pop65p, "PCTAGE65P")
```

### SpatialGrid* Objects
As with the other types of objects, SpatialGrid objects exist with and without associated data frames. The first type (SpatialGridDataFrames) are used for storing raster data (remote sensed images, gridded climate fields, etc). The second type are less useful as they contain no data, only the grid topology. 

The **maptools** package includes a function (`readAsciiGrid()`) for reading raster files in the ESRI ArcInfo format. The file *swiss_dem.grd* contains a raster data set, with a digital elevation model of Switzerland, and can be read in as follows:

```{r fig.keep='none'}
library(maptools)
swiss.dem <- readAsciiGrid("swiss_dem.grd", colname='elev')
spplot(swiss.dem, "elev")
```

As this raster images has a number of missing values (outside Switzerland), we can convert it to a SpatialPixelDataFrame, where the grid topology is retained, but pixels with missing values are deleted. 

```{r results='hide'}
## Convert to SpatialPixelDataFrame
swiss.dem2 <- as(swiss.dem,'SpatialPixelsDataFrame')
summary(swiss.dem2)
```

Note that a lot of R's function for working with gridded data has been moved to the **raster** package - examples of the use of this are given below.

### Writing out spatial data

The `writeOGR()` allows to write spatial data back out as a shapefile. To write out the SpatialPoint* object we made earlier:

```{r eval=FALSE}
writeOGR(wnaclim.spdf, dsn="wnaclim.shp", 
         layer="wnaclim", driver="ESRI Shapefile")
```

## Map projections

R has functions for reprojecting spatial data using the PROJ4 libraries, and functions from the **rgdal** package. Try the following code to reproject a set of sites in Oregon, and the state polygon boundary.

```{r message=FALSE}
ortann <- readOGR("oregon/oregontann.shp")
orotl <- readOGR("oregon/orotl.shp")
proj4string(ortann) <- CRS("+proj=longlat")
proj4string(orotl) <- CRS("+proj=longlat")
```

In this code, we read in the data from shape files (points and polygons), then assign a geographical projection (longitude/latitude) to both objects, using the `CRS()` function. There are several ways to do this. The above code simply sets the projection system to longitude/latitude. An alternative and easier way is to use EPSG codes. These are a standard set of geographic and Cartesian projections, each with a numeric code. The codes can be found [here][epsgID]. The code for a standard long/lat projection, with a WGS84 ellipsoid and datum is 4326, so we could replace the last two line in the previous chunk of code with:

```{r message=FALSE, warning=FALSE}
proj4string(ortann) <- CRS("+init=EPSG:4326")
proj4string(orotl) <- CRS("+init=EPSG:4326")
```

To reproject, we first create a *projection* object that contains the parameters for the new projection as name/value pairs For example '`+proj=aea`' means set the map projection to Albers Equal Area. Other parameters include:

- The name of the projection (`+proj`)
- The two standard parallels (`+lat_1 +lat_2`)
- The coordinates of the projection center (`+lon_0 +lat_0`)
- The ellipsoid (`+GRS80`)
- The units of distance (`+units`)

```{r message=FALSE}
aea.proj <- "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 
  +lon_0=-110 +ellps=GRS80 +datum=NAD83 +units=m"
```

Finally, we use the `spTransform()` function to do the reprojection. This requires a Spatial* object, and our projection system parameters, which are converted to a coordinate reference system using `CRS()`.

```{r message=FALSE}
orotl.proj <- spTransform(orotl, CRS(aea.proj))
ortann.proj <- spTransform(ortann, CRS(aea.proj))
```

If you now compare the coordinates of the projected and unprojected objects, you will see the difference:
```{r results='hide'}
summary(ortann)
summary(ortann.proj)
par(mfrow=c(1,2))
plot(orotl)
plot(ortann, col=2, pch=16, add=TRUE)
plot(orotl.proj)
plot(ortann.proj, col=2, pch=16, add=TRUE)
```

There are many other projections (and parameters) available in the PROJ4 library, and more details and examples can be found [here][projID].

## Visualizing spatial data

The basic plotting function provided with the **sp** package, `spplot()` provides a simple way to map out data with a simple color scale (e.g. `spplot(NY8[,"POP8"])`). This has several arguments that can improve on the basic maps. The following examples demonstrate the use of some of these, and require two extra packages to be installed: **classInt** (provides methods for selecting data intervals for colors) and **RColorBrewer** (provides better color palettes). 

### SpatialPointDataFrame Objects

This next example uses a shapefile of country borders downloaded from [Natural Earth][natEarthID], who provide free global vector and raster data for map making. We will start by loading this:

```{r fig.keep='none'}
countries <- readOGR("ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp")
plot(countries)
```

Now we'll plot the Western North America data set. The additional arguments are:

- `main`: adds a title
- `edge.col`: is the color used for the border of the points
- `sp.layout`: includes other layers to add to the plot. The order they are included here will define the order in which they are added to the map
- `colorkey`: when `TRUE` this forces a continuous color scale. 

```{r}
spplot(wnaclim.spdf,"Jul_Tmp", main="W.NAm July Temp. (deg C)", 
       edge.col=1, sp.layout=list(countries), colorkey=TRUE)
```

We can now change the palette used for the color scale, by choosing one of the **RColorBrewer** palettes. There are several of these, split in sequential, diverging and categorical types. These were developed by Cynthia Brewer specifically for map visualization and include palettes designed for color-blindness, projection and printing. Full details can be at the [ColorBrewer website][colBrewID]. We'll start by loading the package and showing all available palettes:

```{r}
library(RColorBrewer)
display.brewer.all()
```

The names of the palettes are given on left. We will used a diverging palette for the temperature data (feel free to try any of the others as well). To use this with `spplot()` requires two steps. First, we create the new palette, specifying the number of intervals and the name of the palette required. (Note that we reverse this to get red = warm and blue = cold.) Then we add this to the `spplot()` function using the `col.regions` argument. We also need to specify the number of cuts - which is $k-1$, where $k$ is the number of intervals in the palette. 

```{r}
my.pal <- rev(brewer.pal(n = 9, name = "RdYlBu"))
spplot(wnaclim.spdf,"Jul_Tmp", main="W.NAm July Temp. (deg C)",
       sp.layout=list(countries), colorkey=TRUE, edge.col=1, 
       col.regions=my.pal, cuts=8)
```

By default, **RColorBrewer** will only use discrete color palettes. You can, however, create a continuous palette using a combination of `brewer.pal()` and `colorRampPalette()` as follows. This second function takes the ColorBrewer palette and interpolates it to 100 colors:

```{r}
col.levs=100
my.pal.cont = colorRampPalette(my.pal)(col.levs)
spplot(wnaclim.spdf,"Jul_Tmp", main="W.NAm July Temp. (deg C)",
       sp.layout=list(countries), colorkey=TRUE, edge.col=1, 
       col.regions=my.pal.cont, cuts=col.levs-1)
```

### SpatialPolygonDataFrame Objects

We'll now replot the Syracuse shapefile with a better color palette. We'll add an additional function `classIntervals()`, which can be used to change the intervals in a color palette. By default, these are even splits (so each interval covers the same range). If your variable is skewed, then other splits may be better. The `style` argument allows to choose different types of split. Here, we'll use percentiles, which acts in a similar way to a histogram normalization of the variable. Note that we now have three lines of code to make the plot:

- Make the color palette
- Create the intervales
- Make the plot. Note the use of the `at` argument, which sets the breaks to those defined by `classIntervals()`

```{r}
library(classInt)
my.pal <- brewer.pal(n = 7, name = "OrRd")
breaks.qt <- classIntervals(Syracuse$POP8, n = 6, style = "quantile")
spplot(Syracuse, "POP8", col.regions = my.pal, col = "transparent", 
       at = breaks.qt$brks)
```


### SpatialGridDataFrame Objects

And finally an example with a gridded dataset. This is the Swiss DEM used above. We'll also load a shapefile with the Switzerland borders:

```{r}
swiss.bord = readOGR("borders/borders.shp")
my.pal <- rev(brewer.pal(n = 9, name = "Greens"))
breaks.qt <- classIntervals(swiss.dem$elev, n = 8, style = "quantile")
spplot(swiss.dem, "elev", main="Switzerland Elevation",
       sp.layout=list(swiss.bord), col.regions = my.pal, at = breaks.qt$brks)
```

## The **raster** package

The **raster** package offers a array of functions for dealing with gridded data, including the ability to read from many widely used file formats, including remote sensing images (e.g. GeoTiffs) and NetCDF and HDF formats. We will use it here to work with gridded air temperature data (*air.mon.ltm.nc*) from the [NCAR NCEP reanalysis project][ncarID]. This is the long term means for each month from 1981-2010. The file has 12 layers (one per month) and one variable (*air*). 

### Raster objects 
Start by installing and loading the package, the use the `raster()` function to read from the file. 

```{r warning=FALSE, message=FALSE}
library(raster)
r = raster("air.mon.ltm.nc")
```

Note that we have only read the first layer (January). R will tell you that it loaded the variable called *air*. To avoid this message you can specify this directly, which is important for files containing multiple variables:

```{r warning=FALSE, message=FALSE}
r = raster("air.mon.ltm.nc", varname="air")
r
```

`r` is a raster object and has information about grid spacing, coordinates etc. Note that the description of the object tells you whether it is held in memory (small raster files) or on disk. We can make a simple plot:
```{r}
plot(r, main="NCEP NCAR January LTM Tair")
```

And you should be able to see the outline of the continents. By default, NCAR NetCDF files have longitudes running from 0 to 360. We can convert this to the more commonly used (and UK-centric :) -180 to 180 by the function `rotate()`. We can also use a different color palette and add the country shapefile we loaded earlier:

```{r fig.keep='high'}
r = rotate(r)
my.pal <- brewer.pal(n = 9, name = "OrRd")
plot(r, main="NCEP NCAR January LTM Tair", col=my.pal)
plot(countries, add=TRUE)
```

The function `cellStats()` can be used to calculate most summary statistics for a raster layer. So to get the mean global temperature (and standard deviation):
```{r results='hide'}
cellStats(r, mean)
cellStats(r, sd)
```

Values can be extracted from individual locations (or sets of locations) using `extract()`. This can take a set of coordinates in matrix form, or use a Spatial* object. To get the January temperature of Salt Lake City:

```{r results='hide'}
extract(r, cbind(-111.9,40.76))
```

By default this gives you the value of the cell in which the point falls. The value can equally be estimated by bilinear interpolation from the four closest cells with `method='bilinear'`:
```{r results='hide'}
extract(r, cbind(-111.9,40.76), method='bilinear')
```

We created a SpatialPoints object earlier with the location of samples in Western North America (`wnaclim.spdf`). We can now use this, and the raster layer to get the January temperature for all locations:
```{r results='hide'}
extract(r, wnaclim.spdf, method='bilinear')
```

If we want to use only a subset of the original raster layer, the function `crop()` will extract only the cells in a given region. This can be defined using another raster object or Spatial* object, or more simply by defining an `extent` object:

```{r fig.keep='none'}
myext = extent(c(-130,-60,24,50))
r = crop(r, myext)
plot(r, main="NCEP NCAR January LTM Tair", col=my.pal)
plot(countries, add=TRUE)
```

### Raster Stacks

A useful extension to the basic raster functions is the use of stacks. These are a stack of raster layers which represent different variables, but have the same spatial extent and resolution. We can then read in and store all 12 months from the NetCDF file, and then work with this. We read these in with `stack()` and crop them as before:
```{r results='hide'}
myext = extent(c(-130,-60,25,50))
r.stk = rotate(stack("air.mon.ltm.nc", varname="air"))
r.stk = crop(r.stk, myext)
r.stk
```

By typing the name of the stack object, we can see that this has 12 layers, each with 280 cells and the extent, etc. The names attributed to each layer are often unreadable, so we can add our own names:
```{r results='hide'}
names(r.stk) <- paste("TAS",month.abb)
names(r.stk)
```

Now any of the functions we used previously with one layer will be used across all layers. The `plot()` function returns a grid with one plot per layer. Setting the `zlim` argument ensures that all figures use the same range of colors:

```{r fig.keep='none'}
plot(r.stk, col=my.pal, zlim=c(-35,35))
```

Adding a shapefile (or other spatial information) is a little more complex. We create a simple function to plot the country borders, then include this as an `addfun` in the call to `plot()`:
```{r fig.keep='none'}
addBorder = function(){ plot(countries, add=TRUE) }
plot(r.stk, zlim=c(-35,35), col=my.pal, addfun=addBorder)
```

The `cellStats()` function now returns the mean (or other statistic) for all layers, allowing a quick look at the seasonal cycle of average air temperature.

```{r fig.keep='none'}
tavg = cellStats(r.stk, mean)
plot(1:12, tavg, type='l', xlab="Month", ylab="Avg T (C)")
```

And we can do the same for an individual location using `extract()`:
```{r fig.keep='none'}
slc.tavg = extract(r.stk, cbind(-111.9,40.76), method='bilinear')
plot(1:12, slc.tavg, type='l', xlab="Month", ylab="Avg T (C)")
```

This same approch allows you to extract pixels by polygon overlays. Let's re-read the global dataset, and try extracting values for a given country. First, we'll extract China's polygon as a new SpatialPolygon, then use this in the `extract()` function to get all the pixels within the borders

```{r  results='hide'}
r = rotate(raster("air.mon.ltm.nc"))
china.sp = subset(countries, NAME == "China")
extract(r, china.sp)
```

As this function can be used with a set of polygons, the output is in a list. We can use extract the first value of this list, and make a quick histogram:

```{r fig.keep='none'}
china.tjan = extract(r, china.sp)[[1]]
hist(china.tjan)
```

The `extract()` function also takes an argument `fun`. This allows you to calculate a summary statistic for each set of pixels that is extracted (i.e. one per polygon). Here, we'll use this with the countries SpatialPolygon to get an average value of January temperature. We add this back as a new column in the countries object, and then plot it:

```{r warning=FALSE, message=TRUE, fig.keep='none'}
countries$Jan_Tmp = extract(r, countries, fun = mean)[,1]
spplot(countries, "Jan_Tmp", main = "Country average January temperature")
```

## Other spatial functionality

### R and KML files

R can write out KML files for use with Google Earth. A few things to note here:

- The function is `kmlPoints()` from the **maptools** package (there are also functions for KML polygons and lines)
- The function requires a Spatial* object (here a SpatialPointsDataFrame), and the name of an output file
- Optionally we can provide a layer name (`kmlname`) and a vector of names for each point (`name`)
- The optional parameter `description` associates some descriptive text with each point, including HTML tags. 
    + Here we create a series of descriptions that include the January and July temperatures, together with text labels. 
    + Note the use of the `paste()` function to glue together different text strings (which can include numeric vectors)

The second function writes out the points and descriptions into a KML file in the current working directory. Once run, find this file and open it in Google Earth. You can replace the icon using any of the icons found [here][iconID].

```{r eval=FALSE}
description = paste("<b>TJan:</b>", wnaclim.spdf$Jan_Tmp, 
                    "<br><b>TJul:</b>", wnaclim.spdf$Jul_Tmp)
kmlPoints(wnaclim.spdf,"test.kml", kmlname="WNAclim",
          name=wnaclim$WNASEQ, description=description, 
          icon="http://www.google.com/mapfiles/kml/paddle/wht-diamond.png")
```

Here's a second example, with polygon data (the Syracuse polygons). Note that as these are in a UTM projection, we need to reproject to latitude/longitude coordinates, then can write out to a KML file:
```{r eval=FALSE}
ll.proj <- "+proj=longlat +ellps=WGS84"
Syracuse.proj <- spTransform(Syracuse, CRS(ll.proj))

kmlPolygons(Syracuse.proj, kmlfile = "syr.kml", 
            col="white", border="black", lwd=3)
```

## Exercises

The exercise for the module will get you to make some simple maps with new datasets. You should submit the results as an R script with the commands you used, and a Word (or equivalent) with any results or figures *or* as an html page generated from an R Markdown document


### Exercise 1
In the zipfile *countries.zip* is a global shapefile with various socio-economic indicators for different countries. Load this file into R and make plots of any two of the following variables (the variable names are given in brackets). Try different color scales and transformations of the data to get the most informative maps

- Median GDP: (`gdp_md_est`)
- Population: (`pop_est`)
- Income group: (`income_grp`)

### Exercise 2

The NetCDF file *air.mon.mean.nc* contains air temperature data from the NCAR NCEP project, but as mean monthly temperature for every year between 1948 and the present. Using the functions introduced in this lab, read in the data and make a time series of globally average temperature from January 1948 to present. 

----
[projID]: http://trac.osgeo.org/proj/
[ncarID]: http://www.esrl.noaa.gov/psd/data/reanalysis/reanalysis.shtml
[cenID]: http://proximityone.com/cen2010_plfile.htm
[sfID]: https://r-spatial.github.io/sf/articles/sf1.html
[epsgID]: http://spatialreference.org/ref/epsg/
[colBrewID]: http://colorbrewer2.org
[iconID]: https://sites.google.com/site/gmapsdevelopment/
[natEarthID]: https://www.naturalearthdata.com
[tmapID]: https://cran.r-project.org/web/packages/tmap/