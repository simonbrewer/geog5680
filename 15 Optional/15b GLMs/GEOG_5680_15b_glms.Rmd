---
title: "GEOG 5680 6680 15b Generalized linear models in R"
author: | 
  | Simon Brewer
  | Geography Department
  | University of Utah
date: "May 21, 2021"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(png)
library(grid)
```

## Introduction

In this lab we will cover the use of generalized linear models (GLMs) in R for modeling non-normal response variables, e.g. binary outcomes or count data. 

Remember to use RStudio's script editor (top left panel) to enter your commands and then copy-paste these to the console when you are ready. This will keep a copy of the commands that you can save and return to. 

Before starting, remember to create a working directory (e.g. `module15`). Next download the files for today's lab from the Canvas page and move them to this directory (unzip the compressed files). You'll need the following files:

- A dataset from a study of Irish schoolchildren *irished.csv*
- A dataset of honors awards granted to students from three school programs: *hsa.csv*

Then start RStudio, and change your working directory from the [Session] menu > [Set working directory] > [Choose directory...]. Remember to check that R can see the files by running the `list.files()` command. 



## Generalized Linear Models

Generalized Linear Models (GLMs) allow model fitting to non-normal data types and distributions. Previously, we looked at ordinary least squares models, that model some outcome $y$ as a function of one or more covariates $X$:

$$
y = \beta X + \epsilon
$$

We can write this out in a more general way, which simply says that the value of $y$ for any observation is a combination of the expected value ($E(y)$) and the error ($\epsilon$). In other words, any observed value of $y$ is a combination of our best model guess and some leftover error. 

$$
y = E(y) + \epsilon
$$

For our OLS linear model, the expected value is simply a function of the covariates ($X$) and some coefficients ($\beta$), also called the *linear predictor*. 

$$
E(y) = \beta X
$$

And the error is can be described as a normal distribution with a mean of zero and some amount of variance ($\sigma^2$), also know as the error distribution or family:

$$
\epsilon \sim N(0, \sigma^2)
$$

And there's a couple of things we can derive from this:

- As the mean is zero, the model should be unbiased. We have as much chance of over estimating $y$ as underestimating it
- The variance term describes how large these errors might be. As it gets smaller, our model has explained more of the variation in $y$, and there is less that is unexplained

So why is this important, other than to have more equations? This format of decomposing our data into the model part and the error part forms the basis for GLMs. The equation for a GLM follows the same pattern with two small differences:

$$
y = E(y) + \epsilon
$$

The expected value of $y$ is again related the linear predictor, but by using a *link* function. Basically this is a way of transforming $y$ so that the relationship between the *transformed* $y$ and $X$ is linear:

$$
E(y) = g(\mu) = \beta X
$$

The error can now be described as any of a set of probability distributions that are more suitable for non-normal data. 

$$
\epsilon = \mbox{Exponential Distribution}
$$

Fortunately, there are a fairly standard set of default values for this link function and error distribution, depending on the type of data you are going to use. Knowing these, makes modeling non-normal data fairly straightforward, as we will see. 

In R, the `glm()` function is used to fit these models, and follows a similar format to the `lm()` function. The difference is that we need to supply two extra parameters:

- The name of the probability distribution that describes errors (`family`)
- The name of the link function (`link`)

We'll look here at the two most common types of regression: binomial (binary data) and Poisson (count data)

## Binomial Models

As a first example, we will fit a model to some binary data, using a binomial model (these models are also called logistic or logit models). We will use the Irish education dataset and use this to model the relationship between students taking their leaving certificate (`lvcert`) and their scores on a vocal reasoning test (`DVRT`). Download the data from Canvas and load it into R in a data frame called `irished`. 

```{r}
irished <- read.csv("irished.csv")
```

In the file, categorical data (presence/absence of leaving certificate, sex) are coded as numerical values. To convert them into factors for use in R, use the `factor` command as follows:

```{r}
irished$sex <- factor(irished$sex, levels = c(1, 2),
    labels = c("male", "female"))
irished$lvcert <- factor(irished$lvcert, levels = c(0, 1), 
                         labels = c("not taken", "taken"))
```

We will also center the 'DVRT' score:
```{r}
irished$DVRT.cen <- irished$DVRT - mean(irished$DVRT)
```

Next make a boxplot to examine the relationship between whether a student has taken the leaving certificate (variable `lvcert`) and centered DVRT.

```{r fig.keep='high'}
boxplot(DVRT.cen ~ lvcert, data=irished)
```

Which implies that students who take the leaving certificate tend to have higher DVRT scores. What we now want to do is to model the probability that a student will take the leaving certificate, based on their DVRT score. 

Now use the `glm()` function to build a binomial model between variables `lvcert` and centered `DVRT`. For binary variables, the standard error distribution that we use is a binomial, and the link function is a logit transformation, or transformation into log-odds. In the `glm()` function then, we set `family = binomial` and `link='logit'`:

```{r results='hide'}
irished.glm1 <- glm(lvcert ~ DVRT.cen, data = irished, family = binomial(link='logit'))
```

Now let's take a look at the model. We can use the `summary()` function, as we did previously with the linear models:

```{r}
summary(irished.glm1)
```

As the model is based on a logit transformation, the coefficients are given as log-odds. In other words, if the DVRT score increases by a point, the *log-odds* of taking the leaving certificate increase by `r round(coef(irished.glm1)[2], 3)`. To make these a little easier to understand, we can convert them to odds ratios using the `exp()` function:
```{r}
exp(coef(irished.glm1))
```

Now we can interpret the coefficients as odds. The intercept gives the odds of obtaining the leaving certificate for a student with an average DVRT score and is `r round(exp(coef(irished.glm1))[1], 3)` or about 3 to 4. The slope tells us the rate at which the odds change for every unit increase in DVRT scores. These act as a multiplier, so every increase of one point in DVRT score would increase the odds of taking the leaving certificate by a rate of `r round(exp(coef(irished.glm1))[2], 3)`. For example, to get the odds that a student with a DVRT score that is one higher than average takes the certificate, we would multiply `r round(exp(coef(irished.glm1))[1], 3)` by `r round(exp(coef(irished.glm1))[2], 3)` to get the new odds. 

As the original outcome is just a binary variable, we can also estimate the probability of success/failure from out model (in this case, the probability of the presence of having taken the leaving certificate). As an example, we can use the `predict()` function to estimate the probability that a student who has a DVRT score of 120 will take the leaving certificate. As before, we first make a new data.frame a variable called `DVRT.cen` as this was used in the original model. Note that we need to subtract the original mean to make this compatible with the centered DVRT. 

```{r}
newDVRT = data.frame(DVRT.cen=120-mean(irished$DVRT))
predict(irished.glm1, newdata=newDVRT)
```

So that seems like a very high probability. In fact, by default, the `predict()` function will return a transformed value, so this is the log-odds. If we want to see the probability, then we set the argument `type=response`:

```{r}
predict(irished.glm1, newdata=newDVRT, type='response', se.fit = TRUE)
```

The parameter `se.fit=TRUE` tells R to calculate the standard error of prediction with a GLM.

We can also use the `predict()` function to show what the model looks like, by predicting probabilities across a range of DVRT scores. Note that once we have made the prediction, we add the mean back on to the centered DVRT values to plot on the original scale. THe plot we get shows the classic sigmoid shape of a logistic or binomial regression:

```{r}
newDVRT = data.frame(DVRT.cen=seq(60,160)-mean(irished$DVRT))
lvcert.pred <- predict(irished.glm1, 
                       newdata=newDVRT, type='response')
plot(newDVRT$DVRT.cen+mean(irished$DVRT), 
     lvcert.pred, type='l', col=2, lwd=2,
     xlab='DVRT', ylab='Pr(lvcert)')
```

Note that the `anova()` function also works with GLMs, either to test two or more nested models or, as the following example demonstrates, to test a fitted model against a null model. Note that we specify the inference test to obtain a significance level --- the Chi-squared test is used for binomial and Poisson distributions. Here, the very low $p$-value tells us that out model is a significant improvement over the null model. 

```{r}
anova(irished.glm1, test='Chisq')
```

## Poisson Models

Poisson models are used to build relationships with count data. These have a couple of key characteristics: the counts are integers and they are zero-bounded (i.e. you can't have a count less than zero). This example will show you how to build a Poisson model to explore the number of awards won by students. Load the dataset from the file *hsa.csv* into a data frame called `hsa`. As before, the school program (`prog`) is stored as numerical value, so we will need to convert it to a factor. We also center the math score `math`:

```{r}
hsa <- read.csv("hsa.csv")
```

```{r fig.keep='high'}
hsa$prog <- factor(hsa$prog, levels = c(1, 2, 3), 
                   labels = c("General", "Academic", "Vocational"))
hsa$math.cen = hsa$math - mean(hsa$math)
```

Next let's look at the relationships with the number of awards and the type of schools program using boxplots
```{r}
boxplot(math ~ num_awards, data=hsa)
boxplot(math ~ prog, data=hsa)
```

These suggest that the number of awards increases with a students math score, and that awards are generally higher in the academic program that the other programs. 

Now We'll build a Poisson regression model using these two explanatory variables, math score and school program. For count data, the standard error distribution that we use is a Poisson distribution, and the link function is a natural log transformation. In the `glm()` function then, we set `family = poisson` and `link = 'log'`:

```{r}
hsa.glm <- glm(num_awards ~ math.cen + prog, 
               data = hsa, family = poisson(link = 'log'))
summary(hsa.glm)
```

As with the binomial model, the coefficients are for the transformed variable (the log of awards). We can back convert them using `exp()` into something more interpretable:

```{r}
exp(coef(hsa.glm))
```

So how do we interpret this? First, we need to check which school program was used as the reference or baseline (this is the first level in a factor):

```{r}
levels(hsa$prog)
```

So the model is using the `General` program as the reference. We can no use this to interpret the coefficients:

- The number of awards for a student with average math score *from the General program* is `r round(exp(coef(hsa.glm))[1], 3)`
- The number of awards increases at a *rate* of `r round(exp(coef(hsa.glm))[2], 3)` for every point increase in math score
- Students in the `Academic` program tend to have `r round(exp(coef(hsa.glm))[3], 3)` times the number of awards in the `General` program
- Students in the `Vocational` program tend to have `r round(exp(coef(hsa.glm))[3], 3)` times the number of awards in the `General` program

As before, we can predict the expected number of awards obtained by any student. Here, we'll predict for a student from the Academic program, who has a math score of 70. You will again need to make up a data frame for the prediction, however, this time it will need to contain two variables:

```{r results='markup'}
newstudent <- data.frame(math.cen = 70 - mean(hsa$math), 
                         prog = 'Academic')
predict(hsa.glm, newdata = newstudent, 
        type ='response', se.fit = TRUE)
```

Which gives a value a little of 2. Try building a second, simpler model, which only uses the math score to explain the number of awards and use the `anova()` function to compare this to the model that includes both math score and school program. 

## Exercise

There is no additional exercise for today, but you need to submit your R script or (preferably) a Markdown document with the code examples from the lab

## Files used in lab

### Irish Education data set: *irished.csv*
| Column header | Variable |
| --- | --- |
| sex | Sex of student (male = 0; female = 1) |
| DVRT | Vocal reasoning test score |
| fathocc | Prestige score of fathers occupation |
| lvcert | Taken leaving certificate (yes = 1; no = 0) |
| schltype | School type |

### Honor Student Award data set: *hsa.csv*
| Column header | Variable |
| --- | --- |
| id | Student ID |
| num_awards | Number of awards won |
| prog | School program (General = 1; Academic = 2; Vocational = 3) |
| math | Math score |

