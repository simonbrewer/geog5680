---
title: "GEOG 5680 6680 15d Mixed-effects models in R"
author: | 
  | Simon Brewer
  | Geography Department
  | University of Utah
date: "June 1, 2021"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(png)
library(grid)
```

# Introduction

This lab will introduce you to mixed effects (hierarchical) modeling using the **lme4** package. Before starting the lab, make sure this is installed.

Remember to use RStudio's script editor (top left panel) to enter your commands and then copy-paste these to the console when you are ready. This will keep a copy of the commands that you can save and return to. 

Before starting, remember to create a working directory (e.g. `module15`). Next download the files for today's lab from the Canvas page and move them to this directory (unzip the compressed files). You'll need the following file:

- *orthodont.csv*

Then start RStudio, and change your working directory from the [Session] menu > [Set working directory] > [Choose directory...]. Remember to check that R can see the files by running the `list.files()` command. 

# Mixed effects models

Mixed-effects or hierarchical linear models extend basic linear regression models to work with data that have a natural hierarchy or grouping. For example, we may be interested in repeated observations on individuals over time (e.g. growth over time), or in multiple observations from the same organization (e.g. multiple students within multiple schools). Basic linear models assume that all observations are independent, but this is often not the case with grouped data; it is reasonable to assume that all students within school A may more similar to each other (e.g. for a test score) than to students at school B. 

Mixed-effects models account for this by allowing the model coefficients to vary across schools. For example, we might let the intercept (which represents baseline or average test scores) to vary across schools. Further, these models can incorporate group-level characteristic to try and explain this difference. In the school example, we might try to see if the difference in the baseline test scores can be explained using average class-size or other school-level characteristics. 

R has two very good packages for mixed-effects models: **nlme** and **lme4**. We will use the second of these here, so make sure this is installed and loaded. 

```{r}
library(lme4)
```

## Orthodontic data

As an example, we'll use some data of orthodontic measurements on children. This study followed a group of 27 children (16 boys and 11 girls), and measured the distance between the pituitary and the pterygomaxillary fissure in mm based on x-rays. The measurements were taken every two years between the ages of 8 to 14, giving four measurements per child. Let's start by reading these data. 

```{r}
orthodont = read.csv("orthodont.csv")
str(orthodont)
```

Now make a plot of `distance` against age:

```{r}
library(ggplot2)
ggplot(orthodont, aes(x = age, y = distance, col = Subject)) + geom_point()
```

The figure shows a clear trend of increasing distances by age. But you should note that some of the individuals tend to have generally higher distance measurements (e.g. M10):

```{r}
library(dplyr)
orthodont %>% filter(Subject == "M10")
```

# A basic linear model

Let's start by fitting a basic linear model to these data. As a reminder, the basic OLS model is given by 

$$
y = \beta X + \epsilon
$$

For this first model, we'll simply model `distance` as a function of `age`:

```{r}
lm1 = lm(distance ~ age, data = orthodont)
summary(lm1)
```

As a reminder:

- The intercept gives us the expected value when the covariate (`age`) is 0
- The slope is the increase in `distance` with each one unit increase in `age`

To make this a little more interpretable, we'll change the age variable so that zero has a more meaningful value. We'll do that by subtract 8, so a value of 0 in the new variable `age2` indicates a child at the start of the study. 

```{r}
orthodont$age2 = orthodont$age - 8
lm2 = lm(distance ~ age2, data = orthodont)
summary(lm2)
```

So we now the intercept indicates that the expected `distance` for a child entering the study at the age of 8 is `r round(coef(lm2)[1], 2)` mm. The slope is the same as the previous model, and suggests that distance increases by `r round(coef(lm2)[1], 2)` mm for every increase in age of one year.

If we now calculate an ANOVA on the model, we can see that model is significant.

```{r}
anova(lm2)
```

Let's plot out the model:

```{r}
pl <- ggplot(data=orthodont, aes(x=age2,y=distance)) + geom_point() 
pl + geom_line(aes(x=age2,y=predict(lm1)))
```

Which looks fairly reasonable, but if we look at how the model looks relative to `sex`, we can see that the model tends to underestimate `distance` for males, and overestimate for `females`

```{r}
pl + geom_line(aes(x=age2,y=predict(lm1))) + facet_wrap(~Sex)
```

We can go a little further and see how this model relates to the individual children. Here, we subset out four boys and four girls, and plot their `distance` values relative to the model

```{r}
subject.select <- c(paste0("M0",5:8),paste0("F0",2:5))
orthodont.select <- orthodont %>% 
  filter(Subject %in% subject.select)
ggplot(orthodont.select, aes(x=age2,y=distance)) + geom_point() + 
  geom_line(aes(x=age2,y=predict(lm1, newdata=orthodont.select))) + facet_wrap(~Subject, nrow=2) 
```

And again, we can see a problem. While the model might fit well to some of the individuals, for some others it is quite biased (e.g. `M06`).

Indeed, we didn't take into account the fact that the data are repeated measurements made on the same subjects. A more convenient plot for this type of data consists in joining the data of a same individual:

```{r}
ggplot(data=orthodont, aes(x=age2,y=distance)) + geom_point(size=3)  + 
  geom_line(aes(x=age2,y=distance,group=Subject)) + facet_wrap(~Sex)
```

We see on this plot, that even if the distance seems to increase linearly for each individual, the intercept and the slope may change from a subject to another one, including within the same `Sex` group.

# A mixed effects model

Now we'll build a simple mixed-effects models to account for some of the variation we see in this last plot. The *mixed* part of a mixed-effects model implies that we can include a combination of fixed and random effects. Put simply, if we allow a coefficient to vary across groups, this is a random effect. If we calculate a single coefficient across all groups, this is a *fixed* effect, so the OLS model we built before has a fixed intercept and fixed slope. If we allow a coefficient to vary across a group, then it is a *random* effect. 
The two coefficients we can allow to vary here are the intercept and the slope of the regression line against `age`. We'll start by letting the intercept vary, then the slope

## A random intercept model

The equation for our random intercept model for any observation is:

$$
\mbox{distance}_{ij} = \beta_{0j} + \beta_1 \times \mbox{age2} + \epsilon_i
$$

Where $i$ is a single observation and $j$ is a child (this is the 'group'). The random effect part is then given by

$$
\beta_{0j} = \eta_{00} + u
$$

All this means is that the intercept per child is modeled as a normal distribution with mean $\eta_{00}$, which gives the overall population estimate of the intercept. The value of $u$ then measures how much any individual child might vary around that value. An important point arises from this; by comparing $u$, the variance between the groups and $\epsilon$, the variance with the groups, we can get an idea of how different our groups are, and therefore how useful it is to incorporate this random effect. 

So let's build this model in R. The function we will use is `lmer()`, which fits linear mixed-effects models. The syntax for the fixed effects is the same as we have previously seen for `lm()`. The syntax for the random intercepts is `(1 | Subject)`. The `1` tells R to allow the intercepts to vary, and the `| Subject` defines the variable to be used for the groups, here the subject ID. The final argument (`REML = FALSE`) forces the model to be fit by maximum likelihood (I'll discuss why this is important below)

```{r}
lme1 = lmer(distance ~ age2 + (1 | Subject), data = orthodont, REML = FALSE) 
summary(lme1)
```

There's quite a lot more information in the `summary()` output now. A couple of things which are of immediate interest are the random and fixed effects. The fixed effects give a table of coefficients, just as we've seen before, and you might note that these are similar to the values we obtained in the first, linear model. These are the *population* coefficients, and could be used to estimate the `distance` for any new subject given `age`. As the model has a random intercept, the value given here (`r round(fixef(lme1)[1],2)`) is also called the *grand mean intercept*. 

The random effects section provides information on how the variance in the outcome is partitioned between groups (`Subject`; the $u$ term) and within groups (`Residual`; the $\epsilon$ term). Here the variance between groups is over twice that within groups. In other words there variation between the children is over twice the variation in `distance` for a child over the period of study. 

We can also see the individual random effects used to calculate this variance. The `ranef()` function will list these:

```{r}
ranef(lme1)
```

These show the difference from the grand mean intercept for each child in the study. For example, `M10` that we previously noted has having much higher values for distance, has a intercept value of nearly 5 mm above the grand mean. 

We'll now plot the modeled value for each child using the `fitted()` function (this just extracts the modeled value of `distance` for each child and each age)

```{r}
orthodont$random_intercept <- fitted(lme1)
ggplot(data=orthodont, aes(x=age,y=distance)) + geom_point() + 
  geom_line(aes(x=age,y=random_intercept)) + facet_wrap(~Subject, ncol=5) 
```

This looks much better. However, there is one other effect we might want to examine. 

## Random slope model

One other thing we saw from the individual growth curves is that the slope of `distance` against `age` appeared to be steeper for some children and shallower for others. A random slope model attempts to account for this by allowing the slope coefficient to vary across the groups. We're going to fit a model with both random intercept and slope, so our equation becomes

$$
\mbox{distance}_{ij} = \beta_{0j} + \beta_{1j} \times \mbox{age2} + \epsilon_i
$$

Where $i$ is a single observation and $j$ is a child (this is the 'group'). The random intercept is still:

$$
\beta_{0j} = \eta_{00} + u
$$

But we now have a different slope for each child, with the variation in this given by

$$
\beta_{1j} = \eta_{10} + v
$$

Let's build this now. 

```{r}
lme2 = lmer(distance ~ age2 + (age2 | Subject), data = orthodont, REML = FALSE) 
summary(lme2)
```

The output of the `summary()` function should look quite similar to the previous model. Note that the slope coefficient for `age2` is now a population estimate: the grand mean slope.

The random effects now include the variation of the slope coefficients. This is quite small (0.046), just a few percent of the within group variation. One other thing to note here is the correlation between the random effects. This is slightly positive, indicating the children with larger distances at the start of the study tend to have steeper slopes. 

We can again look at the random effects using `ranef()`:

```{r eval = FALSE}
ranef(lme2)
```

Note we now have two columns in the output - the first gives the difference from the grand mean intercept, and the second the difference from the grand mean slope. We can use this output to illustrate the correlation between these effects

```{r}
plot(ranef(lme2)$Subject)
```

And again, we can plot the models for each child:

```{r}
orthodont$random_slope <- fitted(lme2)
ggplot(data=orthodont, aes(x=age,y=distance)) + geom_point() + 
  geom_line(aes(x=age,y=random_slope)) + facet_wrap(~Subject, ncol=5) 
```

As noted above, the variation in the slope is relatively low, but you should see some differences, for example between `M13` and `M14`.

# Mixed-effect model comparison

Mixed-effects models can be compared using the Akaike or Bayesian information criteria (AIC or BIC). These give a measure of the goodness-of-fit of the model (based on the log-likelihood), penalized by the number of model parameters. Note that this comparison is only valid if the models are fit using maximum likelihood, not REML (see below). The AIC (or BIC) don't have a fixed scale (like the r2), so they can only be used in comparison, and the best model is the one that has the lowest AIC. We also include the fixed effects linear model as reference.

```{r}
AIC(lm2, lme1, lme2)
```

Here the AIC scores for the two mixed effects models are virtually identical. Given the small change, we would probably reject the random slope model and keep the first one. 

## Restricted maximum likelihood

The models that we built used maximum likelihood (ML) to fit the model coefficients. We used this as this allows the AIC values to be calculated and the models compared. However, this approach results in a slight bias in the variance estimates for the random effects. This can be fixed by fitting a model using restricted maximum likelihood (REML). In general, the best approach is to start by using ML to allow for model comparison, then refit the chosen model with REML to get an unbiased estimate of the variance. We'll do this now for the first model `lme1`. 

```{r}
lme3 = lmer(distance ~ age2 + (1 | Subject), data = orthodont, REML = TRUE) 
summary(lme3)
```

If you compare these results to those obtained for model `lme`, you should see that the coefficients remain the same, but the variance estimates (`Subject` and `Residual`) have been corrected. 

# Exercise

There is no additional exercise for today, but you need to submit your R script or (preferably) a Markdown document with the code examples from the lab

# Files used in lab

## *orthodont.csv*

Growth curve data on an orthodontic measurement

| Column header | Variable |
| --- | --- |
| distance | distance from the pituitary to the pterygomaxillary fissure (mm) |
| age | Child age at time of measurement |
| Subject | Subject ID |
| Sex | Child sex |

